{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PRE-PROCESSING**\n",
        "\n",
        "* The performance of a machine learning system is heavily dependant on the quality of training data.\n",
        "* So, the first thing we do is to explore the data to get an overview of the variables and the target.\n",
        "* This involves transforming raw data into a format that can be effectively and efficiently used by ML algorithms.\n",
        "* This process ensures the quality, consistency, and readiness of the data, ultimately improving the performance of models.\n",
        "\n",
        "##### **KEY STEPS IN DATA PROCESSING**:\n",
        "\n",
        "* **Data Collection**:\n",
        "  * Gather data from various sources such as databases, CSV files, web scraping, sensors or APIs.\n",
        "  * The collected data often comes in raw, unstructures or semi-structured forms.\n",
        "\n",
        "* **Data Cleaning**:\n",
        "  1. **Handling missing values** - removing rows/cols with missing values and filling with statistical measures using mean, median, mode or using advanced techniques like imputation.\n",
        "  2. **Removing outliers** - outliers can distort the data distribution and affect the model's performance. Techniques like Z-score, IQR, or visualization methods can identify outliers.\n",
        "  3. **Removing Duplicates** - Duplicates create biased situations and hence removed to avoid redundancy.\n",
        "\n",
        "* **Data Transformation:**\n",
        "  1. **Normalization/ Scaking** - adjusts the range of data to specific scale [0-1] to ensure all features contribute equally.\n",
        "  2. **Log Transformation**- used to handle skewed data by transforming highly skewed distributuions into more normally distributed forms.\n",
        "  3. **Binning**- converts numerical values into discrete intervals or bins, which can be useful for handling continuous data.\n",
        "\n",
        "* **Encoding Categorical Data:**\n",
        "  1. **Label Encoding**- converts categorical features into binary columns, useful for ordinal data.\n",
        "  2. **One-Hot Encoding**- converts categorical features into binary columns, each representing each category, making it suitable for nominal data.\n",
        "  3. **Ordinal Encoding**- encodes ordinal features while preserving their order, assigning integers based on category rank.\n",
        "\n",
        "* **Feature Engineering:**\n",
        "  1. **Feature Creation**- creating new features from existing data can improve model performance.\n",
        "  2. **Feature Selection**- reduces dimensionality of data by selecting the most relevant features using methods like correlation, Chi-Swuare and recursive feature elimination(RFE).\n",
        "  3. **Feature Extraction**- tranforms data into a reduced feature set using technoques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD).\n",
        "\n",
        "* **Data Integration:**\n",
        "  * Combines data from multiple sources into a cohesive dataset. It involves merging dataframes, joining tables or concatenating data.\n",
        "\n",
        "* **Data Reduction:**\n",
        "  1. **Dimensionality Reduction**- Reducing the number of input variables through PCA, t-SNE, or feature selection methods, which helps in managing large datasets and reduces computational costs.\n",
        "  2. **Sampling**- Reduces the data size by taking representative samples, especially when working with large datasets.\n",
        "\n",
        "* **Handling Imbalanced Data:**\n",
        "  * Imbalanced datasets, particularly in classification problems, can be balanced using techniques like oversampling (SMOTE), undersampling, or synthetic data generative to ensure each class in representing equally.\n",
        "\n",
        "* **Splitting Dataset:**\n",
        "  * Dividing dataset into training, validation and testing sets to ensure the model can generalize well to unseen data. Common splits include 70-15-15 or 80-20 between training and testing.\n",
        "\n",
        "* **Data Validation:**\n",
        "  * Ensure that data preprocessing has been conducted correctly by cross-checking for data integrity, consistency, and correctness.\n",
        "\n",
        "##### **IMPORTANCE OF DATA PRE-PROCESSING**\n",
        "\n",
        "* Enhances model performance\n",
        "* Reduces Complexity\n",
        "* Prevents Overfitting\n",
        "* Improves Interpretability\n",
        "\n",
        "##### **TOOLS USED**:\n",
        "\n",
        "* Pandas\n",
        "* NumPy\n",
        "* Scikit-Learn\n",
        "* OpenCV\n",
        "* NLTK (for text data) etc"
      ],
      "metadata": {
        "id": "np_Vu0MOpNYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "0Vfx8OrLxvw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. DATA COLLECTION**\n",
        "\n",
        "1. **Pandas**\n",
        "   - **Purpose**: Import data from CSV, Excel, JSON, SQL databases.\n",
        "   - **Common Methods**: `read_csv()`, `read_excel()`, `read_json()`, `read_sql()`.\n",
        "   - **Library**: `pandas`\n",
        "\n",
        "2. **NumPy**\n",
        "   - **Purpose**: Handle and import data from text files (.txt) and other simple data formats.\n",
        "   - **Common Methods**: `loadtxt()`, `genfromtxt()`.\n",
        "   - **Library**: `numpy`\n",
        "\n",
        "3. **Beautiful Soup**\n",
        "   - **Purpose**: Web scraping to collect data from HTML and XML documents.\n",
        "   - **Common Methods**: Parsing and navigating HTML trees.\n",
        "   - **Library**: `beautifulsoup4`\n",
        "\n",
        "4. **Scrapy**\n",
        "   - **Purpose**: Web scraping framework for large-scale data extraction from websites.\n",
        "   - **Common Methods**: `spiders`, `selectors`, handling requests.\n",
        "   - **Library**: `scrapy`\n",
        "\n",
        "5. **Selenium**\n",
        "   - **Purpose**: Automated browser interaction for dynamic content scraping.\n",
        "   - **Common Methods**: Browser automation with drivers like ChromeDriver, FirefoxDriver.\n",
        "   - **Library**: `selenium`\n",
        "\n",
        "6. **Requests**\n",
        "   - **Purpose**: Send HTTP requests to APIs or web pages to gather data.\n",
        "   - **Common Methods**: `get()`, `post()`.\n",
        "   - **Library**: `requests`\n",
        "\n",
        "7. **SQLAlchemy**\n",
        "   - **Purpose**: Data extraction from SQL databases (MySQL, PostgreSQL, SQLite).\n",
        "   - **Common Methods**: `create_engine()`, executing queries.\n",
        "   - **Library**: `sqlalchemy`\n",
        "\n",
        "8. **PyODBC**\n",
        "   - **Purpose**: Access data from ODBC-compliant databases (SQL Server, MS Access).\n",
        "   - **Common Methods**: `connect()`, querying data using SQL.\n",
        "   - **Library**: `pyodbc`\n",
        "\n",
        "9. **PySpark**\n",
        "   - **Purpose**: Data collection and manipulation for large-scale datasets in a distributed environment.\n",
        "   - **Common Methods**: `read.csv()`, `read.json()`.\n",
        "   - **Library**: `pyspark`\n",
        "\n",
        "10. **Tweepy**\n",
        "    - **Purpose**: Collecting data from Twitter via Twitter API.\n",
        "    - **Common Methods**: Accessing tweets, user timelines, etc.\n",
        "    - **Library**: `tweepy`\n",
        "\n",
        "These tools and libraries facilitate the collection of data from various sources, helping to build a robust data foundation for your project."
      ],
      "metadata": {
        "id": "agMiqw-ByatX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "ARr46Q0DynUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. DATA CLEANING**\n",
        "\n",
        "1. **Pandas**\n",
        "   - **Purpose**: Handling missing values, duplicates, data type conversions, outlier detection, and data filtering.\n",
        "   - **Common Methods**:\n",
        "     - `dropna()`: Remove missing values.\n",
        "     - `fillna()`: Fill missing values with specific values or methods (mean, median).\n",
        "     - `drop_duplicates()`: Remove duplicate rows.\n",
        "     - `replace()`: Replace specific values.\n",
        "     - `astype()`: Convert data types.\n",
        "   - **Library**: `pandas`\n",
        "\n",
        "2. **NumPy**\n",
        "   - **Purpose**: Handling missing values, NaNs, and performing data type adjustments for numerical data.\n",
        "   - **Common Methods**:\n",
        "     - `nan_to_num()`: Replace NaNs with numbers.\n",
        "     - `isnan()`: Check for NaN values.\n",
        "   - **Library**: `numpy`\n",
        "\n",
        "3. **Scikit-learn (Impute Module)**\n",
        "   - **Purpose**: Advanced imputation techniques for missing values, including mean, median, and K-Nearest Neighbors imputation.\n",
        "   - **Common Methods**:\n",
        "     - `SimpleImputer()`: Fill missing values using basic strategies.\n",
        "     - `KNNImputer()`: Use nearest neighbors to estimate missing values.\n",
        "   - **Library**: `sklearn.impute`\n",
        "\n",
        "4. **OpenCV**\n",
        "   - **Purpose**: Image cleaning tasks such as noise reduction and smoothing for image datasets.\n",
        "   - **Common Methods**:\n",
        "     - `cv2.GaussianBlur()`: Smooth images to reduce noise.\n",
        "     - `cv2.threshold()`: Adjust pixel values to enhance image quality.\n",
        "   - **Library**: `opencv-python`\n",
        "\n",
        "5. **Pyjanitor**\n",
        "   - **Purpose**: Extends Pandas functionalities for data cleaning tasks, offering easy-to-use chaining methods.\n",
        "   - **Common Methods**:\n",
        "     - `clean_names()`: Clean column names.\n",
        "     - `remove_empty()`: Remove empty rows and columns.\n",
        "   - **Library**: `pyjanitor`\n",
        "\n",
        "6. **SciPy**\n",
        "   - **Purpose**: Advanced statistical cleaning techniques, especially useful in data filtering and smoothing.\n",
        "   - **Common Methods**:\n",
        "     - `zscore()`: Detect and handle outliers.\n",
        "     - `interpolate()`: Fill missing values using interpolation techniques.\n",
        "   - **Library**: `scipy`\n",
        "\n",
        "7. **NLTK (Natural Language Toolkit)**\n",
        "   - **Purpose**: Cleaning and preprocessing text data, including removing stop words, punctuation, and text normalization.\n",
        "   - **Common Methods**:\n",
        "     - `stopwords.words()`: Remove common stop words.\n",
        "     - `word_tokenize()`: Tokenize text into words.\n",
        "   - **Library**: `nltk`\n",
        "\n",
        "8. **Regular Expressions (re module)**\n",
        "   - **Purpose**: Cleaning text data, removing unwanted characters, and formatting data.\n",
        "   - **Common Methods**:\n",
        "     - `re.sub()`: Substitute specific patterns in strings.\n",
        "     - `re.findall()`: Find patterns to identify and clean data.\n",
        "   - **Library**: `re`\n",
        "\n",
        "9. **Dask**\n",
        "   - **Purpose**: Handles large datasets and performs cleaning operations parallelly, scaling Pandas functionalities.\n",
        "   - **Common Methods**:\n",
        "     - `dropna()`, `fillna()`, and other Pandas-like functions for large-scale data.\n",
        "   - **Library**: `dask`\n",
        "\n",
        "10. **Missingno**\n",
        "    - **Purpose**: Visualizes missing data to understand and address missing values effectively.\n",
        "    - **Common Methods**:\n",
        "      - `missingno.matrix()`: Visualize missing data patterns.\n",
        "      - `missingno.heatmap()`: Correlation of missing data.\n",
        "    - **Library**: `missingno`"
      ],
      "metadata": {
        "id": "UFEKqAUzz9el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "EZFTzD7B0Ji5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. DATA TRANSFORMATION**\n",
        "\n",
        "1. **Pandas**\n",
        "   - **Purpose**: Transformation of data through normalization, scaling, encoding categorical variables, and handling datetime formats.\n",
        "   - **Common Methods**:\n",
        "     - `apply()`: Apply custom transformations to data.\n",
        "     - `pivot()`, `melt()`: Reshape data frames.\n",
        "     - `cut()`: Binning data into discrete intervals.\n",
        "     - `astype()`: Convert data types.\n",
        "   - **Library**: `pandas`\n",
        "\n",
        "2. **Scikit-learn (Preprocessing Module)**\n",
        "   - **Purpose**: Scaling, normalization, encoding categorical data, and transforming data distributions.\n",
        "   - **Common Methods**:\n",
        "     - `StandardScaler()`: Standardizes features by removing the mean and scaling to unit variance.\n",
        "     - `MinMaxScaler()`: Scales features to a given range, usually [0, 1].\n",
        "     - `OneHotEncoder()`: Converts categorical variables into binary matrix format.\n",
        "     - `LabelEncoder()`: Converts labels into numeric form.\n",
        "     - `PowerTransformer()`: Stabilizes variance and makes data more Gaussian-like.\n",
        "   - **Library**: `sklearn.preprocessing`\n",
        "\n",
        "3. **NumPy**\n",
        "   - **Purpose**: Performs mathematical transformations, including log transformations and adjustments for skewed data.\n",
        "   - **Common Methods**:\n",
        "     - `log()`, `sqrt()`: Apply logarithmic or square root transformations.\n",
        "     - `reshape()`: Reshape arrays for modeling.\n",
        "   - **Library**: `numpy`\n",
        "\n",
        "4. **SciPy**\n",
        "   - **Purpose**: Advanced transformations like smoothing, interpolation, and normalization.\n",
        "   - **Common Methods**:\n",
        "     - `stats.boxcox()`: Transform data to stabilize variance.\n",
        "     - `interpolate()`: Fill missing data points through interpolation.\n",
        "   - **Library**: `scipy`\n",
        "\n",
        "5. **Category Encoders**\n",
        "   - **Purpose**: Specialized encoding techniques for categorical data, including target encoding, frequency encoding, and binary encoding.\n",
        "   - **Common Methods**:\n",
        "     - `TargetEncoder()`: Encodes categories based on target variable mean.\n",
        "     - `BinaryEncoder()`: Encodes categorical variables into binary digits.\n",
        "   - **Library**: `category_encoders`\n",
        "\n",
        "6. **Feature-engine**\n",
        "   - **Purpose**: Feature transformation, including handling cyclic features, discretization, and rare label encoding.\n",
        "   - **Common Methods**:\n",
        "     - `CyclicalTransformer()`: Transforms cyclic features like time or coordinates.\n",
        "     - `RareLabelEncoder()`: Groups rare labels into a single category.\n",
        "   - **Library**: `feature-engine`\n",
        "\n",
        "7. **TensorFlow / Keras**\n",
        "   - **Purpose**: Data normalization and feature transformation within neural network pipelines.\n",
        "   - **Common Methods**:\n",
        "     - `Normalization()`: Layer for normalizing data in deep learning models.\n",
        "     - `TextVectorization()`: Converts text data into numerical format.\n",
        "   - **Library**: `tensorflow`, `keras`\n",
        "\n",
        "8. **PySpark (MLlib)**\n",
        "   - **Purpose**: Scalable data transformation techniques for big data, including feature scaling and encoding.\n",
        "   - **Common Methods**:\n",
        "     - `StringIndexer()`: Converts categorical columns to numeric indices.\n",
        "     - `VectorAssembler()`: Combines features into a single vector.\n",
        "   - **Library**: `pyspark.ml`\n",
        "\n",
        "9. **Statsmodels**\n",
        "   - **Purpose**: Transformation and manipulation of data in statistical models.\n",
        "   - **Common Methods**:\n",
        "     - `add_constant()`: Adds a constant column for regression modeling.\n",
        "     - `detrend()`: Remove trends from data series.\n",
        "   - **Library**: `statsmodels`\n",
        "\n",
        "10. **XGBoost**\n",
        "    - **Purpose**: Handles missing values and data transformation internally during training.\n",
        "    - **Common Methods**: Automatic handling of missing data and feature importance calculations.\n",
        "    - **Library**: `xgboost`\n",
        "\n",
        "These tools and libraries enable a wide range of data transformation tasks, preparing data to fit the requirements of machine learning algorithms and ensuring that the data is in the best possible form for modeling."
      ],
      "metadata": {
        "id": "J3yojkGr0bV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "auzamUmB0wa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ENCODING CATEGORICAL DATA**\n",
        "\n",
        "1. **Label Encoding**\n",
        "   - **Purpose**: Converts each unique category into an integer value.\n",
        "   - **How It Works**: Assigns each category a unique integer starting from 0. This method is simple but may introduce ordinal relationships where none exist.\n",
        "   - **Common Tools**:\n",
        "     - `LabelEncoder()` from `scikit-learn`.\n",
        "     - `pandas.factorize()`.\n",
        "\n",
        "2. **One-Hot Encoding**\n",
        "   - **Purpose**: Converts each category into a new binary column (1 or 0), where 1 indicates the presence of the category.\n",
        "   - **How It Works**: Creates a binary variable for each category. Effective for nominal data without any intrinsic ordering.\n",
        "   - **Common Tools**:\n",
        "     - `OneHotEncoder()` from `scikit-learn`.\n",
        "     - `pandas.get_dummies()`.\n",
        "\n",
        "3. **Ordinal Encoding**\n",
        "   - **Purpose**: Encodes categories with integers in a specified order.\n",
        "   - **How It Works**: Similar to label encoding but with user-defined ordinal relationships (e.g., low, medium, high).\n",
        "   - **Common Tools**:\n",
        "     - `OrdinalEncoder()` from `scikit-learn`.\n",
        "\n",
        "4. **Frequency Encoding**\n",
        "   - **Purpose**: Encodes categories based on their frequency in the dataset.\n",
        "   - **How It Works**: Replaces each category with the frequency of occurrence within the data, providing a statistical representation.\n",
        "   - **Common Tools**:\n",
        "     - Custom implementation using `pandas`.\n",
        "\n",
        "5. **Target (Mean) Encoding**\n",
        "   - **Purpose**: Encodes categories using the mean of the target variable for each category.\n",
        "   - **How It Works**: Replaces categories with the mean target value associated with each category, useful in supervised learning.\n",
        "   - **Common Tools**:\n",
        "     - `TargetEncoder()` from `category_encoders`.\n",
        "\n",
        "6. **Binary Encoding**\n",
        "   - **Purpose**: Combines label encoding and one-hot encoding to represent categories as binary numbers.\n",
        "   - **How It Works**: Each category is first label-encoded and then converted to binary format, reducing dimensionality compared to one-hot encoding.\n",
        "   - **Common Tools**:\n",
        "     - `BinaryEncoder()` from `category_encoders`.\n",
        "\n",
        "7. **Hashing Encoding**\n",
        "   - **Purpose**: Uses hash functions to convert categories into numerical form, particularly useful for high cardinality features.\n",
        "   - **How It Works**: Applies a hash function to category names, transforming them into fixed-length binary vectors.\n",
        "   - **Common Tools**:\n",
        "     - `HashingEncoder()` from `category_encoders`.\n",
        "\n",
        "8. **Leave-One-Out Encoding**\n",
        "   - **Purpose**: Encodes categories based on the mean target variable, excluding the current observation.\n",
        "   - **How It Works**: Reduces overfitting by encoding using the mean of the target variable for all other observations except the current one.\n",
        "   - **Common Tools**:\n",
        "     - `LeaveOneOutEncoder()` from `category_encoders`.\n",
        "\n",
        "9. **WOE (Weight of Evidence) Encoding**\n",
        "   - **Purpose**: Converts categories into numerical values based on the relationship between the feature and the target variable.\n",
        "   - **How It Works**: Measures the strength of a category’s association with the target variable, often used in binary classification problems.\n",
        "   - **Common Tools**:\n",
        "     - Custom implementations in `pandas` or using `category_encoders`.\n",
        "\n",
        "10. **Polynomial Encoding**\n",
        "    - **Purpose**: Encodes categorical features into polynomial interactions to capture higher-order relationships.\n",
        "    - **How It Works**: Transforms features into interaction terms, allowing models to capture complex relationships.\n",
        "    - **Common Tools**:\n",
        "      - `PolynomialEncoder()` from `category_encoders`.\n",
        "\n",
        "### **Libraries for Encoding Categorical Data**\n",
        "\n",
        "- **Scikit-learn (`sklearn.preprocessing`)**: Provides standard encoders like `LabelEncoder`, `OneHotEncoder`, and `OrdinalEncoder`.\n",
        "- **Pandas**: `get_dummies()` for one-hot encoding and custom implementations for other encodings.\n",
        "- **Category Encoders (`category_encoders`)**: Offers a comprehensive suite of encoders such as target, binary, hashing, WOE, and leave-one-out encoders, especially useful for specialized encoding needs.\n",
        "\n",
        "These encoding methods ensure that categorical data is properly transformed into a numerical format that machine learning models can interpret, preserving the information and relationships within the data."
      ],
      "metadata": {
        "id": "3UdcCxFX0zGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "SzHmmtLA1SJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. FEATURE ENGINEERING**\n",
        "\n",
        "1. **Feature Creation (Domain Knowledge)**\n",
        "   - **Purpose**: Create new features using domain-specific knowledge, combining existing features to derive new insights.\n",
        "   - **Examples**:\n",
        "     - Creating a “Total Spend” feature from “Price” and “Quantity.”\n",
        "     - Generating time-based features like “Day of Week” from a timestamp.\n",
        "\n",
        "2. **Feature Scaling and Normalization**\n",
        "   - **Purpose**: Rescale features to standardize ranges, especially important for algorithms sensitive to feature magnitude.\n",
        "   - **Common Techniques**:\n",
        "     - `StandardScaler()`: Mean=0, variance=1 scaling.\n",
        "     - `MinMaxScaler()`: Scales features to a specific range, usually [0, 1].\n",
        "     - `RobustScaler()`: Scales features using median and IQR to reduce the impact of outliers.\n",
        "   - **Library**: `sklearn.preprocessing`\n",
        "\n",
        "3. **Feature Encoding (Categorical Data)**\n",
        "   - **Purpose**: Convert categorical variables into numerical form.\n",
        "   - **Common Techniques**:\n",
        "     - Label Encoding, One-Hot Encoding, Target Encoding.\n",
        "   - **Library**: `scikit-learn`, `category_encoders`\n",
        "\n",
        "4. **Feature Transformation**\n",
        "   - **Purpose**: Transform data distributions to improve model performance.\n",
        "   - **Common Techniques**:\n",
        "     - Logarithmic, Square Root, and Box-Cox transformations.\n",
        "   - **Library**: `numpy`, `scipy`\n",
        "\n",
        "5. **Feature Selection**\n",
        "   - **Purpose**: Select the most relevant features to improve model performance, reduce complexity, and prevent overfitting.\n",
        "   - **Common Techniques**:\n",
        "     - `SelectKBest()`: Select features based on statistical tests.\n",
        "     - `Recursive Feature Elimination (RFE)`: Recursively remove less important features.\n",
        "     - `Feature Importances from Models`: Using model-based techniques like Random Forest or XGBoost feature importances.\n",
        "   - **Library**: `sklearn.feature_selection`, `xgboost`\n",
        "\n",
        "6. **Dimensionality Reduction**\n",
        "   - **Purpose**: Reduce the number of features while retaining essential information, improving computational efficiency.\n",
        "   - **Common Techniques**:\n",
        "     - Principal Component Analysis (PCA), t-SNE, UMAP.\n",
        "   - **Library**: `sklearn.decomposition`, `umap-learn`\n",
        "\n",
        "7. **Binning**\n",
        "   - **Purpose**: Group continuous variables into discrete intervals or categories.\n",
        "   - **Common Techniques**:\n",
        "     - Equal-width binning, quantile binning, and custom binning based on domain knowledge.\n",
        "   - **Library**: `pandas.cut()`, `pandas.qcut()`\n",
        "\n",
        "8. **Handling Missing Values**\n",
        "   - **Purpose**: Impute or remove missing values to maintain data integrity.\n",
        "   - **Common Techniques**:\n",
        "     - Mean, median, mode imputation, forward/backward fill.\n",
        "     - Advanced imputation using KNN or Iterative Imputer.\n",
        "   - **Library**: `sklearn.impute`, `pandas`\n",
        "\n",
        "9. **Feature Extraction from DateTime**\n",
        "   - **Purpose**: Extract meaningful components from datetime features to capture trends and seasonality.\n",
        "   - **Common Techniques**:\n",
        "     - Extracting year, month, day, hour, minute, weekday, weekend, etc.\n",
        "   - **Library**: `pandas.to_datetime()`\n",
        "\n",
        "10. **Text Feature Engineering**\n",
        "    - **Purpose**: Transform text data into numerical representations.\n",
        "    - **Common Techniques**:\n",
        "      - Bag of Words (BoW), TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "      - Word Embeddings (Word2Vec, GloVe), Sentence Embeddings (BERT).\n",
        "    - **Library**: `scikit-learn`, `nltk`, `spaCy`, `gensim`, `transformers`\n",
        "\n",
        "11. **Polynomial Features**\n",
        "    - **Purpose**: Create new features by combining existing ones with polynomial combinations, capturing non-linear relationships.\n",
        "    - **Common Techniques**:\n",
        "      - `PolynomialFeatures()`: Generate interaction terms and powers of features.\n",
        "    - **Library**: `sklearn.preprocessing`\n",
        "\n",
        "12. **Interaction Features**\n",
        "    - **Purpose**: Create new features by multiplying or combining features to capture interactions between them.\n",
        "    - **Common Techniques**:\n",
        "      - Multiplicative interaction terms or conditional features.\n",
        "    - **Library**: Custom implementation using `pandas`.\n",
        "\n",
        "13. **Time Series Feature Engineering**\n",
        "    - **Purpose**: Extract features from time series data to capture trends, seasonality, and lag effects.\n",
        "    - **Common Techniques**:\n",
        "      - Lag features, rolling windows, expanding windows, and seasonal decomposition.\n",
        "    - **Library**: `pandas`, `statsmodels`\n",
        "\n",
        "14. **Outlier Detection and Handling**\n",
        "    - **Purpose**: Identify and handle outliers that can distort model performance.\n",
        "    - **Common Techniques**:\n",
        "      - Z-score, IQR, and more advanced methods like Isolation Forest.\n",
        "    - **Library**: `scipy`, `sklearn.ensemble`\n",
        "\n",
        "15. **Feature Grouping and Aggregation**\n",
        "    - **Purpose**: Aggregate features by grouping related records to create summary features.\n",
        "    - **Common Techniques**:\n",
        "      - Grouping by categories and calculating sums, means, counts, or other statistical summaries.\n",
        "    - **Library**: `pandas.groupby()`\n",
        "\n",
        "### **Libraries for Feature Engineering**\n",
        "\n",
        "- **Pandas**: Primary library for data manipulation and feature creation, offering comprehensive methods for feature extraction, transformation, and aggregation.\n",
        "- **Scikit-learn**: Provides robust tools for preprocessing, feature selection, scaling, and encoding techniques.\n",
        "- **NumPy**: Essential for numerical transformations, scaling, and handling mathematical operations.\n",
        "- **SciPy**: Used for statistical transformations and advanced data manipulation.\n",
        "- **Category Encoders**: Specialized for encoding categorical features using various encoding strategies.\n",
        "- **Feature-engine**: A library offering transformers for engineering features specifically suited for machine learning pipelines.\n",
        "\n",
        "Feature engineering is the heart of data science, turning raw data into meaningful features that drive predictive modeling, enhancing both accuracy and efficiency of machine learning models."
      ],
      "metadata": {
        "id": "fAzCP5ZU1U7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "jSh-8-jz1aff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. DATA INTEGRATION**\n",
        "\n",
        "1. **Data Merging and Joining**\n",
        "   - **Purpose**: Combine multiple datasets based on common keys or indices, often used to bring related data together.\n",
        "   - **Common Techniques**:\n",
        "     - **Inner Join**: Merges datasets, keeping only rows with matching keys.\n",
        "     - **Outer Join (Full, Left, Right)**: Keeps all rows from one or both datasets, filling in missing values as needed.\n",
        "     - **Concatenation**: Stacks datasets vertically or horizontally.\n",
        "   - **Libraries**:\n",
        "     - `pandas.merge()`\n",
        "     - `pandas.concat()`\n",
        "     - SQL joins in `sqlite3`, `MySQL`, `PostgreSQL`.\n",
        "\n",
        "2. **Data Fusion**\n",
        "   - **Purpose**: Integrates data from multiple sources to produce more consistent, accurate, and useful information.\n",
        "   - **Common Techniques**:\n",
        "     - Sensor fusion (combining data from multiple sensors in IoT).\n",
        "     - Database fusion for integrated reporting.\n",
        "   - **Libraries**:\n",
        "     - Custom fusion algorithms often implemented using `pandas` or `numpy`.\n",
        "\n",
        "3. **Data Blending**\n",
        "   - **Purpose**: Merges data from different sources, focusing on maintaining distinct data sources without altering original data.\n",
        "   - **Common Techniques**:\n",
        "     - Used often in BI tools for quick data combination without ETL processes.\n",
        "   - **Tools**:\n",
        "     - Tableau, Alteryx.\n",
        "\n",
        "4. **ETL (Extract, Transform, Load)**\n",
        "   - **Purpose**: Extracts data from various sources, transforms it into a usable format, and loads it into a target system (e.g., data warehouse).\n",
        "   - **Common Techniques**:\n",
        "     - Extract: SQL queries, API calls.\n",
        "     - Transform: Data cleaning, aggregation, formatting.\n",
        "     - Load: Data insertion into databases or warehouses.\n",
        "   - **Tools**:\n",
        "     - Apache Airflow, Talend, Informatica, Microsoft SSIS, Apache NiFi.\n",
        "\n",
        "5. **Schema Integration**\n",
        "   - **Purpose**: Unifies different data schemas into a single, coherent schema for integrated data access.\n",
        "   - **Common Techniques**:\n",
        "     - Schema matching and mapping, handling schema conflicts.\n",
        "   - **Libraries**:\n",
        "     - `SQLAlchemy` for schema definition and database integration.\n",
        "\n",
        "6. **API Integration**\n",
        "   - **Purpose**: Integrates data from external APIs to enrich existing datasets or obtain real-time information.\n",
        "   - **Common Techniques**:\n",
        "     - RESTful APIs, SOAP APIs.\n",
        "     - Parsing JSON, XML responses.\n",
        "   - **Libraries**:\n",
        "     - `requests`, `http.client`, `urllib` in Python.\n",
        "\n",
        "7. **Data Aggregation**\n",
        "   - **Purpose**: Combines and summarizes data from multiple records or datasets, often used for summarizing large datasets into more manageable forms.\n",
        "   - **Common Techniques**:\n",
        "     - Grouping and aggregation (sum, average, count).\n",
        "   - **Libraries**:\n",
        "     - `pandas.groupby()`, `SQL GROUP BY`.\n",
        "\n",
        "8. **Data Linking and Matching**\n",
        "   - **Purpose**: Connects records from different sources that refer to the same entities, even when identifiers differ.\n",
        "   - **Common Techniques**:\n",
        "     - Fuzzy matching, probabilistic record linkage, entity resolution.\n",
        "   - **Libraries**:\n",
        "     - `fuzzywuzzy`, `dedupe`, `recordlinkage`.\n",
        "\n",
        "9. **Data Consolidation**\n",
        "   - **Purpose**: Combines data from multiple sources into a single, unified dataset, often used in data warehousing.\n",
        "   - **Common Techniques**:\n",
        "     - Aggregation and merging, deduplication of overlapping records.\n",
        "   - **Tools**:\n",
        "     - Data warehouses (Snowflake, Amazon Redshift, Google BigQuery).\n",
        "\n",
        "10. **Data Replication**\n",
        "    - **Purpose**: Copies data from one source to another to ensure consistency across systems, often used in data synchronization.\n",
        "    - **Common Techniques**:\n",
        "      - Batch replication, real-time replication.\n",
        "    - **Tools**:\n",
        "      - AWS Database Migration Service, Oracle GoldenGate.\n",
        "\n",
        "11. **Cloud Data Integration**\n",
        "    - **Purpose**: Integrates data across cloud services, databases, and applications.\n",
        "    - **Common Techniques**:\n",
        "      - Data pipelines for continuous integration and delivery.\n",
        "    - **Tools**:\n",
        "      - Azure Data Factory, Google Cloud Dataflow, AWS Glue.\n",
        "\n",
        "12. **Batch Processing**\n",
        "    - **Purpose**: Processes large volumes of data in scheduled batches, commonly used for periodic data integration.\n",
        "    - **Common Techniques**:\n",
        "      - Batch ETL, scheduled data updates.\n",
        "    - **Tools**:\n",
        "      - Apache Hadoop, Apache Spark.\n",
        "\n",
        "13. **Stream Processing**\n",
        "    - **Purpose**: Processes data in real-time as it flows in, ideal for integrating live data feeds.\n",
        "    - **Common Techniques**:\n",
        "      - Real-time ETL, event stream processing.\n",
        "    - **Tools**:\n",
        "      - Apache Kafka, Apache Flink, Amazon Kinesis.\n",
        "\n",
        "14. **Cross-Database Queries**\n",
        "    - **Purpose**: Executes queries across multiple databases to integrate and compare data without physical merging.\n",
        "    - **Common Techniques**:\n",
        "      - Federated queries, data virtualization.\n",
        "    - **Tools**:\n",
        "      - BigQuery Federation, PrestoDB.\n",
        "\n",
        "### **Libraries and Tools for Data Integration**\n",
        "\n",
        "- **Pandas**: The primary library for merging, joining, and aggregating data in Python.\n",
        "- **SQL Databases (SQLite, MySQL, PostgreSQL)**: For data merging, aggregation, and schema integration through SQL commands.\n",
        "- **Apache Airflow**: Workflow management tool for scheduling ETL pipelines.\n",
        "- **Apache NiFi**: For automating data flows between systems.\n",
        "- **Talend, Informatica**: Popular ETL tools for complex data integration tasks.\n",
        "- **Microsoft Power BI, Tableau**: For data blending and visualization-based integration.\n",
        "- **Apache Spark**: For large-scale data integration tasks including batch and stream processing.\n",
        "\n",
        "Data integration is a crucial step in preparing data for analysis, allowing the consolidation of disparate data sources into a coherent and accessible format, ultimately enhancing data quality and utility for analysis and decision-making."
      ],
      "metadata": {
        "id": "B7B_msjd2Air"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "O0QxP4pu20rX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. DATA REDUCTION**\n",
        "\n",
        "1. **Dimensionality Reduction**\n",
        "   - **Purpose**: Reduce the number of features in the dataset while retaining essential information, making models simpler and faster.\n",
        "   - **Common Techniques**:\n",
        "     - **Principal Component Analysis (PCA)**: Transforms the data to a lower-dimensional space by projecting it onto principal components.\n",
        "     - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Reduces high-dimensional data to two or three dimensions, preserving relationships between data points.\n",
        "     - **Uniform Manifold Approximation and Projection (UMAP)**: Preserves the global and local structure in dimensionality reduction.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.decomposition (PCA)`, `sklearn.manifold (t-SNE)`, `umap-learn`.\n",
        "\n",
        "2. **Feature Selection**\n",
        "   - **Purpose**: Selects a subset of relevant features for model training, reducing dimensionality and improving model performance.\n",
        "   - **Common Techniques**:\n",
        "     - **Filter Methods**: Use statistical tests to select the most relevant features (e.g., ANOVA, Chi-square).\n",
        "     - **Wrapper Methods**: Use algorithms to evaluate subsets of features (e.g., Recursive Feature Elimination - RFE).\n",
        "     - **Embedded Methods**: Features are selected during model training (e.g., Lasso Regression, Tree-based methods).\n",
        "   - **Libraries**:\n",
        "     - `sklearn.feature_selection`, `Boruta`, `xgboost` (feature importance).\n",
        "\n",
        "3. **Data Sampling**\n",
        "   - **Purpose**: Reduce the dataset size by selecting a representative sample, often used when the dataset is too large to process in full.\n",
        "   - **Common Techniques**:\n",
        "     - **Random Sampling**: Selects a random subset of data.\n",
        "     - **Stratified Sampling**: Samples data while preserving the proportion of classes or categories.\n",
        "     - **Systematic Sampling**: Selects data points at regular intervals.\n",
        "   - **Libraries**:\n",
        "     - `pandas.sample()`, `numpy.random.choice()`.\n",
        "\n",
        "4. **Aggregation**\n",
        "   - **Purpose**: Summarizes data by aggregating multiple records into a single record, reducing data size while retaining essential information.\n",
        "   - **Common Techniques**:\n",
        "     - Grouping data and calculating statistical measures (mean, sum, count).\n",
        "     - Rolling window aggregations for time series data.\n",
        "   - **Libraries**:\n",
        "     - `pandas.groupby()`, `pandas.rolling()`.\n",
        "\n",
        "5. **Clustering for Data Reduction**\n",
        "   - **Purpose**: Groups similar data points, allowing for the representation of each cluster by a single point or centroid.\n",
        "   - **Common Techniques**:\n",
        "     - **K-Means Clustering**: Reduces data by summarizing each cluster with its centroid.\n",
        "     - **Hierarchical Clustering**: Builds a hierarchy of clusters that can be pruned to reduce data points.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.cluster`, `scipy.cluster`.\n",
        "\n",
        "6. **Discretization and Binning**\n",
        "   - **Purpose**: Reduces continuous data into discrete bins, which simplifies the data and makes it easier to model.\n",
        "   - **Common Techniques**:\n",
        "     - **Equal-Width Binning**: Divides the data into equal-width intervals.\n",
        "     - **Quantile Binning**: Bins data so that each bin has an equal number of observations.\n",
        "   - **Libraries**:\n",
        "     - `pandas.cut()`, `pandas.qcut()`.\n",
        "\n",
        "7. **Feature Pruning**\n",
        "   - **Purpose**: Removes redundant or less important features based on correlations, variance thresholds, or model performance.\n",
        "   - **Common Techniques**:\n",
        "     - **Variance Threshold**: Removes features with low variance that contribute little to the model.\n",
        "     - **Correlation Analysis**: Drops highly correlated features to avoid redundancy.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.feature_selection.VarianceThreshold`, `pandas.corr()`.\n",
        "\n",
        "8. **Data Compression**\n",
        "   - **Purpose**: Reduces the size of data files through encoding and compression techniques without losing significant information.\n",
        "   - **Common Techniques**:\n",
        "     - **Lossless Compression**: Methods like ZIP or GZIP compress data without losing information.\n",
        "     - **Dimensional Reduction Encoding**: Compresses features using techniques like Singular Value Decomposition (SVD).\n",
        "   - **Libraries**:\n",
        "     - `numpy.linalg.svd`, `gzip`, `blosc`.\n",
        "\n",
        "9. **Instance Selection**\n",
        "   - **Purpose**: Selects the most representative instances or data points, especially useful for large datasets.\n",
        "   - **Common Techniques**:\n",
        "     - Prototype selection algorithms like Condensed Nearest Neighbor (CNN).\n",
        "     - Instance-based learning for reducing training set size.\n",
        "   - **Libraries**:\n",
        "     - Custom implementations using `numpy` or `scikit-learn`.\n",
        "\n",
        "10. **Sparse Data Handling**\n",
        "    - **Purpose**: Reduces the storage and computation needs of sparse matrices, common in text data and high-dimensional spaces.\n",
        "    - **Common Techniques**:\n",
        "      - Removing zero-variance columns, compressing sparse matrices.\n",
        "    - **Libraries**:\n",
        "      - `scipy.sparse`, `sklearn.decomposition.TruncatedSVD`.\n",
        "\n",
        "### **Libraries and Tools for Data Reduction**\n",
        "\n",
        "- **Scikit-learn**: Offers a variety of feature selection, dimensionality reduction, and clustering tools.\n",
        "- **Pandas**: Used extensively for data sampling, aggregation, and manipulation.\n",
        "- **NumPy**: Essential for mathematical operations, compression, and dimensionality reduction.\n",
        "- **SciPy**: Provides advanced mathematical, scientific, and technical data reduction capabilities.\n",
        "- **UMAP-learn**: Used for manifold learning and dimensionality reduction.\n",
        "- **H2O.ai**: For feature selection, reduction, and optimization in large-scale data.\n",
        "\n",
        "Data reduction plays a crucial role in improving the performance of machine learning models by reducing data complexity, processing time, and storage requirements while maintaining the quality and integrity of the information essential for analysis."
      ],
      "metadata": {
        "id": "50GFKKN82uDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "fmXCDXj623Ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. HANDLING IMBALANCED DATA**\n",
        "\n",
        "1. **Resampling Techniques**\n",
        "   - **Purpose**: Adjusts the distribution of the classes to balance the dataset, either by increasing the minority class or decreasing the majority class.\n",
        "   - **Common Techniques**:\n",
        "     - **Oversampling**: Increases the number of minority class samples.\n",
        "       - **Random Oversampling**: Duplicates minority class samples randomly.\n",
        "       - **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for the minority class.\n",
        "     - **Undersampling**: Decreases the number of majority class samples.\n",
        "       - **Random Undersampling**: Randomly removes majority class samples.\n",
        "       - **Tomek Links**: Removes overlapping examples from the majority class.\n",
        "   - **Libraries**:\n",
        "     - `imblearn` (for SMOTE, RandomOverSampler, RandomUnderSampler)\n",
        "     - `sklearn.utils.resample` (for basic resampling techniques)\n",
        "\n",
        "2. **Algorithmic Techniques**\n",
        "   - **Purpose**: Adjusts the learning algorithm to handle imbalanced data more effectively.\n",
        "   - **Common Techniques**:\n",
        "     - **Class Weights**: Assigns different weights to classes to penalize misclassifications of the minority class more.\n",
        "     - **Anomaly Detection**: Treats the minority class as an anomaly or outlier and uses algorithms designed for anomaly detection.\n",
        "   - **Libraries**:\n",
        "     - `sklearn` (for class_weight parameter in models like `LogisticRegression`, `RandomForestClassifier`)\n",
        "     - `pyod` (for anomaly detection)\n",
        "\n",
        "3. **Ensemble Methods**\n",
        "   - **Purpose**: Combines multiple models to improve classification performance on imbalanced datasets.\n",
        "   - **Common Techniques**:\n",
        "     - **Balanced Random Forest**: Uses balanced bootstrapped samples for training.\n",
        "     - **EasyEnsemble**: Combines multiple models using undersampling and oversampling.\n",
        "     - **AdaBoost**: Boosts the minority class examples by adjusting weights iteratively.\n",
        "   - **Libraries**:\n",
        "     - `imblearn.ensemble` (for BalancedRandomForestClassifier, EasyEnsembleClassifier)\n",
        "     - `sklearn.ensemble` (for AdaBoostClassifier)\n",
        "\n",
        "4. **Synthetic Data Generation**\n",
        "   - **Purpose**: Generates new data points to balance the classes.\n",
        "   - **Common Techniques**:\n",
        "     - **SMOTE (Synthetic Minority Over-sampling Technique)**: Creates synthetic samples for the minority class by interpolation.\n",
        "     - **ADASYN (Adaptive Synthetic Sampling)**: An extension of SMOTE that adapts to the data distribution.\n",
        "   - **Libraries**:\n",
        "     - `imblearn.over_sampling.SMOTE`\n",
        "     - `imblearn.over_sampling.ADASYN`\n",
        "\n",
        "5. **Data Augmentation**\n",
        "   - **Purpose**: Enhances the dataset by creating variations of existing samples.\n",
        "   - **Common Techniques**:\n",
        "     - **Image Augmentation**: Applies transformations like rotation, flipping, or scaling to images to increase diversity.\n",
        "     - **Text Augmentation**: Uses techniques like synonym replacement or back-translation to create diverse text samples.\n",
        "   - **Libraries**:\n",
        "     - `keras.preprocessing.image.ImageDataGenerator` (for image augmentation)\n",
        "     - `nltk` or `textaugment` (for text augmentation)\n",
        "\n",
        "6. **Threshold Moving**\n",
        "   - **Purpose**: Adjusts the decision threshold for classifying the minority class.\n",
        "   - **Common Techniques**:\n",
        "     - **Probability Threshold Adjustment**: Changes the threshold for assigning a sample to the minority class.\n",
        "     - **Precision-Recall Curve**: Adjusts the threshold based on precision-recall trade-offs.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.metrics` (for precision-recall curves and threshold adjustments)\n",
        "\n",
        "7. **Cost-sensitive Learning**\n",
        "   - **Purpose**: Modifies the cost function of the learning algorithm to account for class imbalance.\n",
        "   - **Common Techniques**:\n",
        "     - **Cost-sensitive Classification**: Adds costs to the misclassification of the minority class in the cost function.\n",
        "   - **Libraries**:\n",
        "     - Custom cost functions in `sklearn`, `xgboost`, `lightgbm`, etc.\n",
        "\n",
        "8. **Evaluation Metrics Adjustment**\n",
        "   - **Purpose**: Uses evaluation metrics that better reflect the performance on imbalanced data.\n",
        "   - **Common Metrics**:\n",
        "     - **F1 Score**: Balances precision and recall.\n",
        "     - **ROC-AUC**: Measures the area under the receiver operating characteristic curve.\n",
        "     - **Precision-Recall AUC**: Focuses on the trade-off between precision and recall.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.metrics` (for F1 score, ROC-AUC, Precision-Recall AUC)\n",
        "\n",
        "9. **Hybrid Methods**\n",
        "   - **Purpose**: Combines various techniques for a more robust solution to class imbalance.\n",
        "   - **Common Techniques**:\n",
        "     - **Combining Resampling and Algorithmic Adjustments**: Uses oversampling with class weighting.\n",
        "     - **Ensemble Methods with Resampling**: Combines ensemble techniques with data resampling.\n",
        "   - **Libraries**:\n",
        "     - `imblearn` (for hybrid resampling techniques and ensemble methods)\n",
        "\n",
        "10. **Visualization Techniques**\n",
        "    - **Purpose**: Visualizes the impact of class imbalance and the effectiveness of handling methods.\n",
        "    - **Common Techniques**:\n",
        "      - **Confusion Matrix**: Shows true positives, false positives, true negatives, and false negatives.\n",
        "      - **ROC Curve and Precision-Recall Curve**: Visualizes model performance with different thresholds.\n",
        "    - **Libraries**:\n",
        "      - `sklearn.metrics` (for confusion matrix, ROC curve, Precision-Recall curve)\n",
        "      - `seaborn`, `matplotlib` (for visualization)\n",
        "\n",
        "### **Libraries and Tools for Handling Imbalanced Data**\n",
        "\n",
        "- **Scikit-learn**: Offers tools for class weighting, threshold adjustment, and evaluation metrics.\n",
        "- **Imbalanced-learn (imblearn)**: Provides advanced resampling techniques, ensemble methods, and tools for handling class imbalance.\n",
        "- **PyOD**: Specializes in anomaly detection methods.\n",
        "- **Keras/TensorFlow**: Includes image augmentation tools and can implement custom loss functions for cost-sensitive learning.\n",
        "- **XGBoost, LightGBM**: Support for cost-sensitive learning and handling imbalanced data.\n",
        "\n",
        "Handling imbalanced data effectively is crucial for building robust machine learning models, ensuring that minority classes are well-represented and that the model’s performance is not biased towards the majority class."
      ],
      "metadata": {
        "id": "eRbWkkKg3J8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "0rxZ6gdq3QyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. SPLITTING DATASET**\n",
        "\n",
        "1. **Train-Test Split**\n",
        "   - **Purpose**: Divides the dataset into training and testing sets to evaluate model performance.\n",
        "   - **Common Techniques**:\n",
        "     - **Simple Random Split**: Randomly divides the dataset into training and testing sets.\n",
        "     - **Proportional Split**: Maintains the proportion of classes in the training and testing sets.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.train_test_split`\n",
        "\n",
        "2. **K-Fold Cross-Validation**\n",
        "   - **Purpose**: Splits the dataset into k subsets (folds) to train and validate the model k times, providing a more robust performance evaluation.\n",
        "   - **Common Techniques**:\n",
        "     - **K-Fold**: Divides the dataset into k equal-sized folds, each used as a validation set once.\n",
        "     - **Stratified K-Fold**: Ensures each fold has the same proportion of class labels as the entire dataset.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.KFold`, `sklearn.model_selection.StratifiedKFold`\n",
        "\n",
        "3. **Leave-One-Out Cross-Validation (LOOCV)**\n",
        "   - **Purpose**: Uses a single observation as the validation set and the remaining observations as the training set, repeating this process for each observation.\n",
        "   - **Common Techniques**:\n",
        "     - **LOOCV**: A special case of k-fold cross-validation where k equals the number of data points.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.LeaveOneOut`\n",
        "\n",
        "4. **Leave-P-Out Cross-Validation**\n",
        "   - **Purpose**: Similar to LOOCV, but leaves out p observations for validation at each iteration.\n",
        "   - **Common Techniques**:\n",
        "     - **Leave-P-Out**: Uses p observations as the validation set and the rest as the training set.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.LeavePOut`\n",
        "\n",
        "5. **Time Series Split**\n",
        "   - **Purpose**: Used for time series data to maintain temporal order while splitting the data into training and testing sets.\n",
        "   - **Common Techniques**:\n",
        "     - **TimeSeriesSplit**: Provides train-test splits in a time series fashion, respecting the temporal sequence of data.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.TimeSeriesSplit`\n",
        "\n",
        "6. **Train-Validation-Test Split**\n",
        "   - **Purpose**: Splits the dataset into three sets—training, validation, and testing—enabling model tuning and evaluation.\n",
        "   - **Common Techniques**:\n",
        "     - **Triple Split**: Divides the dataset into training, validation, and testing sets.\n",
        "     - **Stratified Split**: Maintains class proportions across all three sets.\n",
        "   - **Libraries**:\n",
        "     - Manual implementation using `sklearn.model_selection.train_test_split`\n",
        "\n",
        "7. **Bootstrap Sampling**\n",
        "   - **Purpose**: Creates multiple training datasets by sampling with replacement from the original dataset, often used for ensemble methods.\n",
        "   - **Common Techniques**:\n",
        "     - **Bootstrap Aggregation (Bagging)**: Combines multiple bootstrapped samples to improve model robustness.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.utils.resample`\n",
        "\n",
        "8. **Group K-Fold Cross-Validation**\n",
        "   - **Purpose**: Ensures that samples from the same group are not split between training and testing sets, useful when data is grouped.\n",
        "   - **Common Techniques**:\n",
        "     - **GroupKFold**: Ensures that the same group is only in one fold.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.GroupKFold`\n",
        "\n",
        "9. **Stratified Sampling**\n",
        "   - **Purpose**: Ensures that each split of the dataset has the same proportion of class labels as the full dataset, especially useful for imbalanced datasets.\n",
        "   - **Common Techniques**:\n",
        "     - **Stratified Sampling**: Maintains class distribution in each split.\n",
        "   - **Libraries**:\n",
        "     - `sklearn.model_selection.StratifiedShuffleSplit`, `sklearn.model_selection.StratifiedKFold`\n",
        "\n",
        "10. **Random Sampling**\n",
        "    - **Purpose**: Randomly selects a subset of the dataset for training or testing, used in various splitting methods.\n",
        "    - **Common Techniques**:\n",
        "      - **Random Split**: Simple random selection for creating training and testing sets.\n",
        "    - **Libraries**:\n",
        "      - `pandas.DataFrame.sample()`, `numpy.random.choice()`\n",
        "\n",
        "### **Libraries and Tools for Splitting Datasets**\n",
        "\n",
        "- **Scikit-learn**: Provides a wide range of functions for splitting datasets, including `train_test_split`, `KFold`, `StratifiedKFold`, `TimeSeriesSplit`, `LeaveOneOut`, `GroupKFold`, and `StratifiedShuffleSplit`.\n",
        "- **Pandas**: Useful for manual dataset splitting using `DataFrame.sample()`.\n",
        "- **NumPy**: Used for random sampling and generating indices for splitting.\n",
        "- **Scipy**: May be used for more advanced sampling methods.\n",
        "\n",
        "Splitting datasets appropriately is crucial for evaluating machine learning models effectively, ensuring they generalize well to unseen data, and providing robust performance metrics."
      ],
      "metadata": {
        "id": "zcQKz2AZ_PJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "mrn0VqteBEeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. DATA VALIDATION**\n",
        "\n",
        "1. **Schema Validation**\n",
        "   - **Purpose**: Ensures that the data adheres to a predefined schema, including data types, formats, constraints, and ranges.\n",
        "   - **Common Techniques**:\n",
        "     - **Data Type Checks**: Verifies that each field is of the expected data type (e.g., integer, float, string).\n",
        "     - **Range Checks**: Confirms that numeric values fall within a specific range.\n",
        "     - **Constraint Validation**: Ensures adherence to constraints such as unique values, primary keys, and foreign keys.\n",
        "   - **Libraries**:\n",
        "     - `pandas` (for `dtype` checks and constraint validations)\n",
        "     - `pydantic` (for schema validation with models)\n",
        "     - `marshmallow` (for object serialization and validation)\n",
        "\n",
        "2. **Data Consistency Checks**\n",
        "   - **Purpose**: Validates the logical consistency of the data within the dataset.\n",
        "   - **Common Techniques**:\n",
        "     - **Cross-Field Validation**: Ensures related fields are consistent (e.g., start date is before the end date).\n",
        "     - **Uniqueness Checks**: Checks that certain fields, such as IDs or email addresses, are unique.\n",
        "     - **Referential Integrity**: Verifies that foreign keys match the primary keys in related tables.\n",
        "   - **Libraries**:\n",
        "     - `pandas` (for `duplicated()` and `merge()` checks)\n",
        "     - `SQLAlchemy` (for referential integrity in databases)\n",
        "\n",
        "3. **Data Completeness Validation**\n",
        "   - **Purpose**: Ensures no missing or incomplete data within critical fields of the dataset.\n",
        "   - **Common Techniques**:\n",
        "     - **Null Checks**: Identifies missing values using methods like `isnull()` or `isna()`.\n",
        "     - **Threshold Checks**: Sets acceptable thresholds for missing data and flags if exceeded.\n",
        "   - **Libraries**:\n",
        "     - `pandas` (for `isnull()`, `notnull()`)\n",
        "     - `missingno` (for visualizing missing data patterns)\n",
        "\n",
        "4. **Data Format Validation**\n",
        "   - **Purpose**: Verifies that data is in the correct format, such as date formats, numerical formats, or regular expressions for strings.\n",
        "   - **Common Techniques**:\n",
        "     - **Regular Expression Matching**: Checks strings against predefined patterns (e.g., email, phone number).\n",
        "     - **Date Format Validation**: Ensures dates conform to a specific format (e.g., YYYY-MM-DD).\n",
        "   - **Libraries**:\n",
        "     - `re` (Python’s regex library for pattern matching)\n",
        "     - `datetime` (for validating and parsing date formats)\n",
        "     - `dateutil` (for extended date parsing capabilities)\n",
        "\n",
        "5. **Outlier Detection**\n",
        "   - **Purpose**: Identifies data points that are significantly different from the majority, which may indicate errors or anomalies.\n",
        "   - **Common Techniques**:\n",
        "     - **Z-Score and IQR (Interquartile Range)**: Detects outliers based on statistical thresholds.\n",
        "     - **Isolation Forest and DBSCAN**: Machine learning techniques for anomaly detection.\n",
        "   - **Libraries**:\n",
        "     - `scipy.stats` (for Z-Score and IQR)\n",
        "     - `sklearn.ensemble.IsolationForest`\n",
        "     - `sklearn.cluster.DBSCAN`\n",
        "\n",
        "6. **Business Rule Validation**\n",
        "   - **Purpose**: Validates data against specific business rules and logic unique to the domain or use case.\n",
        "   - **Common Techniques**:\n",
        "     - **Custom Rule Checks**: Applies domain-specific rules, such as ensuring age is non-negative or inventory is not negative.\n",
        "     - **Conditional Validation**: Checks based on business conditions (e.g., if status is ‘Closed’, end date must be filled).\n",
        "   - **Libraries**:\n",
        "     - `pandas` (custom rule implementation)\n",
        "     - `cerberus` (for rule-based data validation)\n",
        "\n",
        "7. **Data Quality Validation**\n",
        "   - **Purpose**: Measures the overall quality of the data by checking for accuracy, consistency, completeness, and timeliness.\n",
        "   - **Common Techniques**:\n",
        "     - **Accuracy Checks**: Compares data with trusted sources or benchmarks.\n",
        "     - **Timeliness Checks**: Ensures that data is up-to-date and within the expected timeframe.\n",
        "   - **Libraries**:\n",
        "     - `pandas-profiling` (for comprehensive data quality reports)\n",
        "     - `great_expectations` (for setting data quality expectations and validation)\n",
        "\n",
        "8. **Cross-Validation of Data Sources**\n",
        "   - **Purpose**: Validates data by comparing multiple sources or versions of the dataset.\n",
        "   - **Common Techniques**:\n",
        "     - **Source Cross-Check**: Compares data fields across different datasets to ensure alignment.\n",
        "     - **Reconciliation**: Matches records between primary and secondary data sources.\n",
        "   - **Libraries**:\n",
        "     - `pandas` (for merging and comparing datasets)\n",
        "     - `datacompy` (for data comparison and reconciliation)\n",
        "\n",
        "9. **Statistical Validation**\n",
        "   - **Purpose**: Uses statistical methods to ensure data distributions meet expectations, often used in time series or financial data.\n",
        "   - **Common Techniques**:\n",
        "     - **Distribution Checks**: Validates that data follows expected statistical distributions (e.g., normal distribution).\n",
        "     - **Hypothesis Testing**: Applies tests like t-tests or chi-squared tests to validate data assumptions.\n",
        "   - **Libraries**:\n",
        "     - `scipy.stats` (for statistical tests)\n",
        "     - `numpy` (for distribution checks)\n",
        "\n",
        "10. **Visual Validation**\n",
        "    - **Purpose**: Uses data visualization techniques to spot inconsistencies, outliers, or patterns that indicate validation issues.\n",
        "    - **Common Techniques**:\n",
        "      - **Scatter Plots and Histograms**: Visualize distributions and spot outliers.\n",
        "      - **Box Plots**: Identify data spread and potential outliers.\n",
        "    - **Libraries**:\n",
        "      - `matplotlib`, `seaborn` (for plotting and visualization)\n",
        "\n",
        "### **Libraries and Tools for Data Validation**\n",
        "\n",
        "- **Pandas**: Widely used for most data validation tasks, including checking data types, missing values, duplicates, and custom validation logic.\n",
        "- **Pydantic**: Provides data validation and settings management using Python type annotations.\n",
        "- **Cerberus**: A lightweight and flexible data validation library.\n",
        "- **Great Expectations**: A powerful tool for creating expectations on data, validating, and documenting data pipelines.\n",
        "- **Marshmallow**: Used for converting complex data types to and from native Python data types and validating data.\n",
        "- **Scikit-learn**: Provides statistical validation and anomaly detection capabilities.\n",
        "- **SciPy and NumPy**: Used for statistical validation checks and calculations.\n",
        "\n",
        "Data validation ensures the integrity, accuracy, and reliability of data, making it crucial for data-driven decision-making and robust machine learning models."
      ],
      "metadata": {
        "id": "tGNCpfiBBGc4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytt0D4gs2Hl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}